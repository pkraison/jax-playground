{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfvQ0Ok3WjuHbVX+0f2kYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkraison/jax-playground/blob/main/Training_a_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switch accelerator by clicking on `Runtime` in the top menu, then `Change runtime type`, and selecting `GPU` from the `Hardware accelerator` dropdown. If the runtime fails, feel free to disable the GPU and run the notebook on the CPU.\n",
        "\n",
        "JAX provides a high-performance backend with the XLA (Accelerated Linear Algebra) compiler to optimize our computations on the available hardware. As JAX continue to be developed, there are more and more features being implemented, that improve efficiency. We can enable some of these new features via XLA flags. At the moment of writing (JAX version 0.4.25, March 2024), the following flags are recommended in the JAX [GPU performance tips tutorial](https://jax.readthedocs.io/en/latest/gpu_performance_tips.html#xla-performance-flags) and [PAX](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/rosetta/projects/pax/README.md#xla-flags):"
      ],
      "metadata": {
        "id": "H7k1YL8q5w1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vjkk5DiMqMDk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = (\n",
        "    \"--xla_gpu_enable_triton_softmax_fusion=true \"\n",
        "    \"--xla_gpu_triton_gemm_any=false \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, Dict, Tuple\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.struct import dataclass\n",
        "from flax.training import train_state\n",
        "\n",
        "# Type aliases\n",
        "PyTree = Any\n",
        "Metrics = Dict[str, Tuple[jax.Array, ...]]"
      ],
      "metadata": {
        "id": "cHc-0SpB7w6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixed Precision Training\n",
        "\n",
        "Mixed precision training is a technique that uses both 16-bit and 32-bit floating-point numbers to speed up training. The idea is to use 16-bit floating-point numbers for most of the computations, as they are faster and require less memory. However, 16-bit floating-point numbers have a smaller range and precision compared to 32-bit floating-point numbers. Therefore, we use 32-bit floating-point numbers for certain computations, such as the model's weight updates and the final loss computation, to avoid numerical instability.\n",
        "\n",
        "A potential problem with `float16` is that we can encounter underflow and overflow issues during training. This means that the gradients or activations become too large or too small to be represented in the range of `float16`, and we lose information. Scaling the loss and gradients by a constant factor can help mitigate this issue to bring the values back into the representable range. This is known as [loss scaling](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling), and it is a common technique used in mixed precision training.\n",
        "\n",
        "As an alternative, JAX and other deep learning frameworks like [PyTorch](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) also support the `bfloat16` format, which is a 16-bit floating-point format with 8 exponent bits and 7 mantissa bits. The `bfloat16` format has a larger range but lower precision compared to the IEEE half-precision type `float16`, and matches `float32` in terms of range. A closer comparison between the formats is shown in the figure below (figure credit: [Google Cloud Documentation](https://cloud.google.com/tpu/docs/bfloat16)):\n",
        "\n",
        "\n",
        "The main benefit of using `bfloat16` is that it can be used without loss scaling, as it has a larger range compared to `float16`. This allows `bfloat16` to be used as a drop-in replacement for `float32` in many cases to save memory and achieve performances close to `float32` (see e.g. [JKalamkar et al., 2019](https://arxiv.org/abs/1905.12322)). For situations where precision matters over range, `float16` may be the better option. Besides memory efficiency, many accelerators like [TPUs](https://cloud.google.com/tpu/docs/bfloat16) and [GPUs](https://www.nvidia.com/en-us/data-center/tensor-cores/) have native support for `bfloat16`, which can lead up to 2x speedup in training performance compared to `float32` on these devices. Hence, we will use `bfloat16` in this notebook.\n",
        "\n",
        "We implement mixed precision training by lowering all features and activations within the model to `bfloat16`, while keeping the weights and optimizer states in `float32`. This is done to keep high precision for the weight updates and optimizer states, while reducing the memory footprint and increasing the training speed by using `bfloat16` for the forward and backward passes. While this does not reduce the memory footprint of the model parameters themselves, we often achieve a significant reduction in memory consumption due to the reduced memory footprint of the activations without influencing the model's performance."
      ],
      "metadata": {
        "id": "0GoltG9i7-3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    dtype: Any\n",
        "    hidden_size: int = 256\n",
        "    num_classes: int = 100\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
        "        x = nn.Dense(\n",
        "            features=self.hidden_size,\n",
        "            dtype=self.dtype,  # Computation in specified dtype, params stay in float32\n",
        "        )(x)\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        x = nn.silu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)\n",
        "        x = nn.Dense(\n",
        "            features=self.num_classes,\n",
        "            dtype=self.dtype,\n",
        "        )(x)\n",
        "        x = x.astype(jnp.float32)\n",
        "        x = nn.log_softmax(x, axis=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WrzPyvve9GhM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.ones((512, 128), dtype=jnp.float32)\n",
        "rngs = {\"params\": jax.random.PRNGKey(0), \"dropout\": jax.random.PRNGKey(1)}\n",
        "model_float32 = MLPClassifier(dtype=jnp.float32)\n",
        "model_float32.tabulate(rngs, x, train=True, console_kwargs={\"force_jupyter\": True})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "hGEoTHdpMTbP",
        "outputId": "ce759c19-3665-4200-e095-14be5cc8a4d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                      MLPClassifier Summary                                       \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - \u001b[2mfloat32\u001b[0m[512,128] │ \u001b[2mfloat32\u001b[0m[512,100] │                          │\n",
              "│             │               │ - train: True      │                  │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,128]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[128,256] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m33,024 \u001b[0m\u001b[1;2m(132.1 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                  │ scale: \u001b[2mfloat32\u001b[0m[256]      │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m             │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,100] │ bias: \u001b[2mfloat32\u001b[0m[100]       │\n",
              "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[256,100] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m25,700 \u001b[0m\u001b[1;2m(102.8 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
              "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
              "\u001b[1m                                                                                                  \u001b[0m\n",
              "\u001b[1m                               Total Parameters: 59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m                                \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      MLPClassifier Summary                                       </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path        </span>┃<span style=\"font-weight: bold\"> module        </span>┃<span style=\"font-weight: bold\"> inputs             </span>┃<span style=\"font-weight: bold\"> outputs          </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │                          │\n",
              "│             │               │ - train: True      │                  │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,256] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">33,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(132.1 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]      │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>             │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[100]       │\n",
              "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256,100] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">25,700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(102.8 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">             </span>│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">            Total </span>│<span style=\"font-weight: bold\"> 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">        </span>│\n",
              "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                                                  </span>\n",
              "<span style=\"font-weight: bold\">                               Total Parameters: 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">                                </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDpEkoq8MUSX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}