{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFK4aje2hf0cu6p20idiJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkraison/jax-playground/blob/main/Training_a_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switch accelerator by clicking on `Runtime` in the top menu, then `Change runtime type`, and selecting `GPU` from the `Hardware accelerator` dropdown. If the runtime fails, feel free to disable the GPU and run the notebook on the CPU.\n",
        "\n",
        "JAX provides a high-performance backend with the XLA (Accelerated Linear Algebra) compiler to optimize our computations on the available hardware. As JAX continue to be developed, there are more and more features being implemented, that improve efficiency. We can enable some of these new features via XLA flags. At the moment of writing (JAX version 0.4.25, March 2024), the following flags are recommended in the JAX [GPU performance tips tutorial](https://jax.readthedocs.io/en/latest/gpu_performance_tips.html#xla-performance-flags) and [PAX](https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/rosetta/projects/pax/README.md#xla-flags):"
      ],
      "metadata": {
        "id": "H7k1YL8q5w1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vjkk5DiMqMDk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"XLA_FLAGS\"] = (\n",
        "    \"--xla_gpu_enable_triton_softmax_fusion=true \"\n",
        "    \"--xla_gpu_triton_gemm_any=false \"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from pprint import pprint\n",
        "from typing import Any, Callable, Dict, Tuple\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "from flax.struct import dataclass\n",
        "from flax.training import train_state\n",
        "\n",
        "# Type aliases\n",
        "PyTree = Any\n",
        "Metrics = Dict[str, Tuple[jax.Array, ...]]"
      ],
      "metadata": {
        "id": "cHc-0SpB7w6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixed Precision Training\n",
        "\n",
        "Mixed precision training is a technique that uses both 16-bit and 32-bit floating-point numbers to speed up training. The idea is to use 16-bit floating-point numbers for most of the computations, as they are faster and require less memory. However, 16-bit floating-point numbers have a smaller range and precision compared to 32-bit floating-point numbers. Therefore, we use 32-bit floating-point numbers for certain computations, such as the model's weight updates and the final loss computation, to avoid numerical instability.\n",
        "\n",
        "A potential problem with `float16` is that we can encounter underflow and overflow issues during training. This means that the gradients or activations become too large or too small to be represented in the range of `float16`, and we lose information. Scaling the loss and gradients by a constant factor can help mitigate this issue to bring the values back into the representable range. This is known as [loss scaling](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling), and it is a common technique used in mixed precision training.\n",
        "\n",
        "As an alternative, JAX and other deep learning frameworks like [PyTorch](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) also support the `bfloat16` format, which is a 16-bit floating-point format with 8 exponent bits and 7 mantissa bits. The `bfloat16` format has a larger range but lower precision compared to the IEEE half-precision type `float16`, and matches `float32` in terms of range. A closer comparison between the formats is shown in the figure below (figure credit: [Google Cloud Documentation](https://cloud.google.com/tpu/docs/bfloat16)):\n",
        "\n",
        "\n",
        "The main benefit of using `bfloat16` is that it can be used without loss scaling, as it has a larger range compared to `float16`. This allows `bfloat16` to be used as a drop-in replacement for `float32` in many cases to save memory and achieve performances close to `float32` (see e.g. [JKalamkar et al., 2019](https://arxiv.org/abs/1905.12322)). For situations where precision matters over range, `float16` may be the better option. Besides memory efficiency, many accelerators like [TPUs](https://cloud.google.com/tpu/docs/bfloat16) and [GPUs](https://www.nvidia.com/en-us/data-center/tensor-cores/) have native support for `bfloat16`, which can lead up to 2x speedup in training performance compared to `float32` on these devices. Hence, we will use `bfloat16` in this notebook.\n",
        "\n",
        "We implement mixed precision training by lowering all features and activations within the model to `bfloat16`, while keeping the weights and optimizer states in `float32`. This is done to keep high precision for the weight updates and optimizer states, while reducing the memory footprint and increasing the training speed by using `bfloat16` for the forward and backward passes. While this does not reduce the memory footprint of the model parameters themselves, we often achieve a significant reduction in memory consumption due to the reduced memory footprint of the activations without influencing the model's performance."
      ],
      "metadata": {
        "id": "0GoltG9i7-3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPClassifier(nn.Module):\n",
        "    dtype: Any\n",
        "    hidden_size: int = 256\n",
        "    num_classes: int = 100\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
        "        x = nn.Dense(\n",
        "            features=self.hidden_size,\n",
        "            dtype=self.dtype,  # Computation in specified dtype, params stay in float32\n",
        "        )(x)\n",
        "        x = nn.LayerNorm(dtype=self.dtype)(x)\n",
        "        x = nn.silu(x)\n",
        "        x = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(x)\n",
        "        x = nn.Dense(\n",
        "            features=self.num_classes,\n",
        "            dtype=self.dtype,\n",
        "        )(x)\n",
        "        x = x.astype(jnp.float32)\n",
        "        x = nn.log_softmax(x, axis=-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WrzPyvve9GhM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.ones((512, 128), dtype=jnp.float32)\n",
        "rngs = {\"params\": jax.random.PRNGKey(0), \"dropout\": jax.random.PRNGKey(1)}\n",
        "model_float32 = MLPClassifier(dtype=jnp.float32)\n",
        "model_float32.tabulate(rngs, x, train=True, console_kwargs={\"force_jupyter\": True})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "hGEoTHdpMTbP",
        "outputId": "ce759c19-3665-4200-e095-14be5cc8a4d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                      MLPClassifier Summary                                       \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - \u001b[2mfloat32\u001b[0m[512,128] │ \u001b[2mfloat32\u001b[0m[512,100] │                          │\n",
              "│             │               │ - train: True      │                  │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,128]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[128,256] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m33,024 \u001b[0m\u001b[1;2m(132.1 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                  │ scale: \u001b[2mfloat32\u001b[0m[256]      │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m             │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,256]   │ \u001b[2mfloat32\u001b[0m[512,100] │ bias: \u001b[2mfloat32\u001b[0m[100]       │\n",
              "│             │               │                    │                  │ kernel: \u001b[2mfloat32\u001b[0m[256,100] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ \u001b[1m25,700 \u001b[0m\u001b[1;2m(102.8 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
              "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
              "\u001b[1m                                                                                                  \u001b[0m\n",
              "\u001b[1m                               Total Parameters: 59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m                                \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                      MLPClassifier Summary                                       </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path        </span>┃<span style=\"font-weight: bold\"> module        </span>┃<span style=\"font-weight: bold\"> inputs             </span>┃<span style=\"font-weight: bold\"> outputs          </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │                          │\n",
              "│             │               │ - train: True      │                  │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,256] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">33,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(132.1 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                  │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]      │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>             │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,256]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[100]       │\n",
              "│             │               │                    │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256,100] │\n",
              "│             │               │                    │                  │                          │\n",
              "│             │               │                    │                  │ <span style=\"font-weight: bold\">25,700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(102.8 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼──────────────────┼──────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">             </span>│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">            Total </span>│<span style=\"font-weight: bold\"> 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">        </span>│\n",
              "└─────────────┴───────────────┴────────────────────┴──────────────────┴──────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                                                  </span>\n",
              "<span style=\"font-weight: bold\">                               Total Parameters: 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">                                </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_bfloat16 = MLPClassifier(dtype=jnp.bfloat16)\n",
        "model_bfloat16.tabulate(rngs, x, train=True, console_kwargs={\"force_jupyter\": True})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "bDpEkoq8MUSX",
        "outputId": "44419992-4c4c-45aa-9bc0-da4e94b01c74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m                                       MLPClassifier Summary                                       \u001b[0m\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mpath       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                  \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - \u001b[2mfloat32\u001b[0m[512,128] │ \u001b[2mfloat32\u001b[0m[512,100]  │                          │\n",
              "│             │               │ - train: True      │                   │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ \u001b[2mfloat32\u001b[0m[512,128]   │ \u001b[2mbfloat16\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                   │ kernel: \u001b[2mfloat32\u001b[0m[128,256] │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ \u001b[1m33,024 \u001b[0m\u001b[1;2m(132.1 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,256] │ bias: \u001b[2mfloat32\u001b[0m[256]       │\n",
              "│             │               │                    │                   │ scale: \u001b[2mfloat32\u001b[0m[256]      │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ \u001b[1m512 \u001b[0m\u001b[1;2m(2.0 KB)\u001b[0m             │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ \u001b[2mbfloat16\u001b[0m[512,256]  │ \u001b[2mbfloat16\u001b[0m[512,100] │ bias: \u001b[2mfloat32\u001b[0m[100]       │\n",
              "│             │               │                    │                   │ kernel: \u001b[2mfloat32\u001b[0m[256,100] │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ \u001b[1m25,700 \u001b[0m\u001b[1;2m(102.8 KB)\u001b[0m        │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│\u001b[1m \u001b[0m\u001b[1m           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
              "└─────────────┴───────────────┴────────────────────┴───────────────────┴──────────────────────────┘\n",
              "\u001b[1m                                                                                                   \u001b[0m\n",
              "\u001b[1m                                Total Parameters: 59,236 \u001b[0m\u001b[1;2m(236.9 KB)\u001b[0m\u001b[1m                                \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                       MLPClassifier Summary                                       </span>\n",
              "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> path        </span>┃<span style=\"font-weight: bold\"> module        </span>┃<span style=\"font-weight: bold\"> inputs             </span>┃<span style=\"font-weight: bold\"> outputs           </span>┃<span style=\"font-weight: bold\"> params                   </span>┃\n",
              "┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│             │ MLPClassifier │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,100]  │                          │\n",
              "│             │               │ - train: True      │                   │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dense_0     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,128]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                   │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[128,256] │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ <span style=\"font-weight: bold\">33,024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(132.1 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ LayerNorm_0 │ LayerNorm     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]       │\n",
              "│             │               │                    │                   │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256]      │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ <span style=\"font-weight: bold\">512 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.0 KB)</span>             │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dropout_0   │ Dropout       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256] │                          │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│ Dense_1     │ Dense         │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">bfloat16</span>[512,100] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[100]       │\n",
              "│             │               │                    │                   │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[256,100] │\n",
              "│             │               │                    │                   │                          │\n",
              "│             │               │                    │                   │ <span style=\"font-weight: bold\">25,700 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(102.8 KB)</span>        │\n",
              "├─────────────┼───────────────┼────────────────────┼───────────────────┼──────────────────────────┤\n",
              "│<span style=\"font-weight: bold\">             </span>│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">             Total </span>│<span style=\"font-weight: bold\"> 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">        </span>│\n",
              "└─────────────┴───────────────┴────────────────────┴───────────────────┴──────────────────────────┘\n",
              "<span style=\"font-weight: bold\">                                                                                                   </span>\n",
              "<span style=\"font-weight: bold\">                                Total Parameters: 59,236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(236.9 KB)</span><span style=\"font-weight: bold\">                                </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient checkpointing is a technique that trades compute for memory by recomputing some activations during the backward pass. The idea is to store only a subset of the activations during the forward pass, and recompute the rest of the activations during the backward pass. This can be useful when the memory consumption of the activations is the limiting factor for the model's size, and the recomputation of the activations is cheaper than storing them. This is often the case for models with a large memory footprint, such as the Transformer, where the activations can be a significant portion of the memory consumption.\n",
        "In JAX and Flax, we can implement gradient checkpointing using the `remat` function. The `remat` function allows us to control which intermediate arrays should be saved on the forward pass, and which are recomputed on the backward pass. As a simple example, consider the following function that computes the GELU activation function manually with its approximation (see e.g. [Hendrycks and Gimpel, 2016](https://arxiv.org/abs/1606.08415)). Note that in practice, we would use the `gelu` function from the `flax.nn` module which is already optimized, but we use this example to illustrate the concept of gradient checkpointing:\n"
      ],
      "metadata": {
        "id": "aB-GqIbWVlZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x: jax.Array) -> jax.Array:\n",
        "    \"\"\"GeLU activation function with approximate tanh.\"\"\"\n",
        "    # This will be printed once every time the function is executed.\n",
        "    jax.debug.print(\"Executing GeLU\")\n",
        "    # See https://arxiv.org/abs/1606.08415 for details.\n",
        "    x3 = jnp.power(x, 3)\n",
        "    tanh_input = np.sqrt(2 / np.pi) * (x + 0.044715 * x3)\n",
        "    return 0.5 * x * (1 + jnp.tanh(tanh_input))"
      ],
      "metadata": {
        "id": "EMVrmWJLUT5Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this function, we instantiate several intermediate tensors, which we may need to store during the backward pass and can be expensive for large tensors. Meanwhile, the computation is relatively cheap, such that we would want to compute these tensors during the backward pass instead of storing them. We can use the `remat` function to control which tensors are stored and which are recomputed during the backward pass. We can use the `remat` function as follows:"
      ],
      "metadata": {
        "id": "SjetukH6V4mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(x: jax.Array, remat: bool) -> jax.Array:\n",
        "    act_fn = gelu\n",
        "    if remat:\n",
        "        act_fn = jax.remat(act_fn)\n",
        "    return jnp.mean(act_fn(x))"
      ],
      "metadata": {
        "id": "zMQYxadhUbcR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we now transform this function with a `jax.grad` call, we will see that JAX is executing the function twice (we see the `Executing GeLU` print statement twice). This is because JAX is computing the forward pass, then releases all intermediate tensors, and then recomputes them again in the backward pass."
      ],
      "metadata": {
        "id": "4Yjel1IeV-Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jax.random.normal(jax.random.PRNGKey(0), (100,))\n",
        "grad_fn = jax.grad(loss_fn)\n",
        "_ = grad_fn(x, remat=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKFtr7-wUgkq",
        "outputId": "0ca452f7-0904-46e3-f459-bc43909d75c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GeLU\n",
            "Executing GeLU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we would run the same function without the `remat` function, we would only see the `Executing GeLU` print statement once, as JAX would not need to recompute the intermediate tensors during the backward pass."
      ],
      "metadata": {
        "id": "kqiqbu0CWFdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = loss_fn(x, remat=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spvY9QYWUju3",
        "outputId": "cc99f324-4a8e-4671-d34c-31f4e458bcf8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing GeLU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that the `remat` function is controlling which tensors are stored and which are recomputed during the backward pass. We will see in the later Transformer example how we can use it in a neural network layer.\n",
        "\n",
        "In JAX, the XLA compiler can also automatically apply rematerialization to the forward pass when we jit the function. In that case, we do not need to use the `remat` function explicitly, as the XLA compiler will automatically apply rematerialization to the forward pass. However, it can still be beneficial to use the `remat` function in some cases, like in `scans` (see [practical notes on remat](https://jax.readthedocs.io/en/latest/notebooks/autodiff_remat.html#practical-notes)) or to manually control which tensors are stored and which are recomputed."
      ],
      "metadata": {
        "id": "RB0YwQAxWHPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common trade-off in training large models is the batch size. A larger batch size can lead to a more accurate estimate of the gradient, but it also requires more memory. In some cases, the batch size is limited by the memory of the accelerator, and we cannot increase the batch size further. In these cases, we can use gradient accumulation to simulate a larger batch size by accumulating the gradients over multiple sub-batches. Each sub-batch is independently processed, and we perform an optimizer step once all sub-batches have been processed. Gradient accumulation can be useful when the memory consumption of the activations is the limiting factor for the model's size, but we require a larger batch size for training. However, a disadvantage of gradient accumulation is that each sub-batch is processed independently and sequentially, such that nothing is parallelized and we need to ensure that we can still utilize the accelerator to its full potential with the small batch size.\n",
        "\n"
      ],
      "metadata": {
        "id": "AuOKtck9Wmjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In JAX and Flax, we have easy control over the gradient accumulation process, since we explicitly calculate the gradients via `jax.grad`. Let's implement this process for our simple classification MLP from the mixed precision training. We first create a train state from Flax, which we extend by an RNG for easier handling of dropout."
      ],
      "metadata": {
        "id": "-P2bVQayWvqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "    rng: jax.Array"
      ],
      "metadata": {
        "id": "YiRMh9DFUnCT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Batch:\n",
        "    inputs: jax.Array\n",
        "    labels: jax.Array"
      ],
      "metadata": {
        "id": "a2j2rmj8WWA3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_loss_fn(\n",
        "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
        ") -> Tuple[PyTree, Metrics]:\n",
        "    \"\"\"Classification loss function with cross-entropy.\"\"\"\n",
        "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": rng})\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
        "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
        "    batch_size = batch.inputs.shape[0]\n",
        "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
        "    loss = loss.mean()\n",
        "    return loss, step_metrics"
      ],
      "metadata": {
        "id": "5VBKnBKpWY5H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_gradients_loop(\n",
        "    state: TrainState,\n",
        "    batch: Batch,\n",
        "    rng: jax.random.PRNGKey,\n",
        "    num_minibatches: int,\n",
        "    loss_fn: Callable,\n",
        ") -> Tuple[PyTree, Metrics]:\n",
        "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
        "\n",
        "    Args:\n",
        "        state: Current training state.\n",
        "        batch: Full training batch.\n",
        "        rng: Random number generator to use.\n",
        "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
        "        loss_fn: Loss function to calculate gradients and metrics.\n",
        "\n",
        "    Returns:\n",
        "        Tuple with accumulated gradients and metrics over the minibatches.\n",
        "    \"\"\"\n",
        "    batch_size = batch.inputs.shape[0]\n",
        "    minibatch_size = batch_size // num_minibatches\n",
        "    rngs = jax.random.split(rng, num_minibatches)\n",
        "    # Define gradient function for single minibatch.\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    # Prepare loop variables.\n",
        "    grads = None\n",
        "    metrics = None\n",
        "    for minibatch_idx in range(num_minibatches):\n",
        "        with jax.named_scope(f\"minibatch_{minibatch_idx}\"):\n",
        "            # Split the batch into minibatches.\n",
        "            start = minibatch_idx * minibatch_size\n",
        "            end = start + minibatch_size\n",
        "            minibatch = jax.tree_map(lambda x: x[start:end], batch)\n",
        "            # Calculate gradients and metrics for the minibatch.\n",
        "            (_, step_metrics), step_grads = grad_fn(\n",
        "                state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
        "            )\n",
        "            # Accumulate gradients and metrics across minibatches.\n",
        "            if grads is None:\n",
        "                grads = step_grads\n",
        "                metrics = step_metrics\n",
        "            else:\n",
        "                grads = jax.tree_map(jnp.add, grads, step_grads)\n",
        "                metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
        "    # Average gradients over minibatches.\n",
        "    grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
        "    return grads, metrics"
      ],
      "metadata": {
        "id": "DXBiEim1Wb7D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A disadvantage of the implementation above is that we need to compile the gradient function for each sub-batch, which can be slow. We can avoid this by using the `scan` transformation in JAX ([docs](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html)), which allows us to write a for-loop with a single compilation of the inner step. The `scan` transformation requires the function to take two inputs: the `carry` and the input `x`. The `carry` is the state that is passed between the steps, and the `x` input is the input to the current step. The function returns the new `carry` and any output that we want to gather per step. In our case, the `carry` is the accumulated gradients and the accumulated metrics of all previous steps, and the `x` input is the current minibatch index, with which we select the minibatch and RNG to use. As the new carry, we return the updated accumulated gradients and metrics, and do not require a per-step output. We implement the gradient accumulation with `scan` below:"
      ],
      "metadata": {
        "id": "mA8CyzNSW5oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_gradients_scan(\n",
        "    state: TrainState,\n",
        "    batch: Batch,\n",
        "    rng: jax.random.PRNGKey,\n",
        "    num_minibatches: int,\n",
        "    loss_fn: Callable,\n",
        ") -> Tuple[PyTree, Metrics]:\n",
        "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
        "\n",
        "    In this version, we use `jax.lax.scan` to loop over the minibatches. This is more efficient in terms of compilation time.\n",
        "\n",
        "    Args:\n",
        "        state: Current training state.\n",
        "        batch: Full training batch.\n",
        "        rng: Random number generator to use.\n",
        "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
        "        loss_fn: Loss function to calculate gradients and metrics.\n",
        "\n",
        "    Returns:\n",
        "        Tuple with accumulated gradients and metrics over the minibatches.\n",
        "    \"\"\"\n",
        "    batch_size = batch.inputs.shape[0]\n",
        "    minibatch_size = batch_size // num_minibatches\n",
        "    rngs = jax.random.split(rng, num_minibatches)\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "    def _minibatch_step(minibatch_idx: jax.Array | int) -> Tuple[PyTree, Metrics]:\n",
        "        \"\"\"Determine gradients and metrics for a single minibatch.\"\"\"\n",
        "        minibatch = jax.tree_map(\n",
        "            lambda x: jax.lax.dynamic_slice_in_dim(  # Slicing with variable index (jax.Array).\n",
        "                x, start_index=minibatch_idx * minibatch_size, slice_size=minibatch_size, axis=0\n",
        "            ),\n",
        "            batch,\n",
        "        )\n",
        "        (_, step_metrics), step_grads = grad_fn(\n",
        "            state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
        "        )\n",
        "        return step_grads, step_metrics\n",
        "\n",
        "    def _scan_step(\n",
        "        carry: Tuple[PyTree, Metrics], minibatch_idx: jax.Array | int\n",
        "    ) -> Tuple[Tuple[PyTree, Metrics], None]:\n",
        "        \"\"\"Scan step function for looping over minibatches.\"\"\"\n",
        "        step_grads, step_metrics = _minibatch_step(minibatch_idx)\n",
        "        carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
        "        return carry, None\n",
        "\n",
        "    # Determine initial shapes for gradients and metrics.\n",
        "    grads_shapes, metrics_shape = jax.eval_shape(_minibatch_step, 0)\n",
        "    grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
        "    metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
        "    # Loop over minibatches to determine gradients and metrics.\n",
        "    (grads, metrics), _ = jax.lax.scan(\n",
        "        _scan_step, init=(grads, metrics), xs=jnp.arange(num_minibatches), length=num_minibatches\n",
        "    )\n",
        "    # Average gradients over minibatches.\n",
        "    grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
        "    return grads, metrics"
      ],
      "metadata": {
        "id": "emZmjucgW1EI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accumulate_gradients(*args, use_scan: bool = False, **kwargs) -> Tuple[PyTree, Metrics]:\n",
        "    if use_scan:\n",
        "        return accumulate_gradients_scan(*args, **kwargs)\n",
        "    else:\n",
        "        return accumulate_gradients_loop(*args, **kwargs)"
      ],
      "metadata": {
        "id": "wRKHB7GEW-Ua"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(\n",
        "    state: TrainState,\n",
        "    metrics: Metrics | None,\n",
        "    batch: Batch,\n",
        "    num_minibatches: int,\n",
        ") -> Tuple[TrainState, Metrics]:\n",
        "    \"\"\"Training step function.\n",
        "\n",
        "    Executes a full training step with gradient accumulation.\n",
        "\n",
        "    Args:\n",
        "        state: Current training state.\n",
        "        metrics: Current metrics, accumulated from previous training steps.\n",
        "        batch: Training batch.\n",
        "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
        "\n",
        "    Returns:\n",
        "        Tuple with updated training state (parameters, optimizer state, etc.) and metrics.\n",
        "    \"\"\"\n",
        "    # Split the random number generator for the current step.\n",
        "    rng, step_rng = jax.random.split(state.rng)\n",
        "    # Determine gradients and metrics for the full batch.\n",
        "    grads, step_metrics = accumulate_gradients(\n",
        "        state, batch, step_rng, num_minibatches, loss_fn=classification_loss_fn, use_scan=True\n",
        "    )\n",
        "    # Optimizer step.\n",
        "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
        "    # Accumulate metrics across training steps.\n",
        "    if metrics is None:\n",
        "        metrics = step_metrics\n",
        "    else:\n",
        "        metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
        "    return new_state, metrics"
      ],
      "metadata": {
        "id": "go0LFlo9XBeg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "num_inputs = 128\n",
        "num_classes = 100\n",
        "rng_seed = 0\n",
        "\n",
        "rng = jax.random.PRNGKey(rng_seed)\n",
        "data_input_rng, data_label_rng, model_rng, state_rng = jax.random.split(rng, 4)\n",
        "batch = Batch(\n",
        "    inputs=jax.random.normal(data_input_rng, (batch_size, num_inputs)),\n",
        "    labels=jax.random.randint(data_label_rng, (batch_size,), 0, num_classes),\n",
        ")"
      ],
      "metadata": {
        "id": "xW6sRVIQXD3k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero dropout for checking later equality between training with and without gradient accumulation.\n",
        "model = MLPClassifier(dtype=jnp.bfloat16, dropout_rate=0.0)\n",
        "params = model.init(model_rng, batch.inputs, train=False)[\"params\"]\n",
        "state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=optax.adam(1e-3),\n",
        "    rng=state_rng,\n",
        ")"
      ],
      "metadata": {
        "id": "hoeLWRAuXKfB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, metric_shapes = jax.eval_shape(\n",
        "    functools.partial(train_step, num_minibatches=4),\n",
        "    state,\n",
        "    None,\n",
        "    batch,\n",
        ")\n",
        "print(\"Metric shapes:\")\n",
        "pprint(metric_shapes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DGBKowYXNpO",
        "outputId": "e44e4b3e-1170-46b0-88a9-844fb2bc0b36"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n",
            "<ipython-input-14-b24f61c18315>:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
            "<ipython-input-14-b24f61c18315>:51: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric shapes:\n",
            "{'accuracy': (ShapeDtypeStruct(shape=(), dtype=int32),\n",
            "              ShapeDtypeStruct(shape=(), dtype=int32)),\n",
            " 'loss': (ShapeDtypeStruct(shape=(), dtype=float32),\n",
            "          ShapeDtypeStruct(shape=(), dtype=int32))}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-b24f61c18315>:45: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
            "<ipython-input-14-b24f61c18315>:57: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_jit = jax.jit(\n",
        "    train_step,\n",
        "    static_argnames=\"num_minibatches\",\n",
        ")"
      ],
      "metadata": {
        "id": "eQh6yRQOXWGA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_minibatches(\n",
        "    state: TrainState,\n",
        "    batch: Batch,\n",
        "    num_minibatches: int,\n",
        "    num_train_steps: int,\n",
        ") -> Tuple[TrainState, Metrics]:\n",
        "    \"\"\"Small helper function for training loop.\"\"\"\n",
        "    train_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
        "    for _ in range(num_train_steps):\n",
        "        state, train_metrics = train_step_jit(state, train_metrics, batch, num_minibatches)\n",
        "    return state, train_metrics"
      ],
      "metadata": {
        "id": "B2eR8qEJaFA7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics(metrics: Metrics, title: str | None = None) -> None:\n",
        "    \"\"\"Prints metrics with an optional title.\"\"\"\n",
        "    metrics = jax.device_get(metrics)\n",
        "    lines = [f\"{k}: {v[0] / v[1]:.6f}\" for k, v in metrics.items()]\n",
        "    if title:\n",
        "        title = f\" {title} \"\n",
        "        max_len = max(len(title), max(map(len, lines)))\n",
        "        lines = [title.center(max_len, \"=\")] + lines\n",
        "    print(\"\\n\".join(lines))"
      ],
      "metadata": {
        "id": "TprUpEc9aM59"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_mini1, metrics_mini1 = train_with_minibatches(\n",
        "    state, batch, num_minibatches=1, num_train_steps=5\n",
        ")\n",
        "state_mini4, metrics_mini4 = train_with_minibatches(\n",
        "    state, batch, num_minibatches=4, num_train_steps=5\n",
        ")\n",
        "print_metrics(metrics_mini1, \"Minibatch 1\")\n",
        "print_metrics(metrics_mini4, \"Minibatch 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7flAPDniaQRf",
        "outputId": "e9c86ab3-291c-4486-f507-830a7ca1e0ec"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-0d040d5692bd>:8: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  train_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n",
            "<ipython-input-14-b24f61c18315>:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
            "<ipython-input-14-b24f61c18315>:51: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n",
            "<ipython-input-14-b24f61c18315>:45: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
            "<ipython-input-14-b24f61c18315>:57: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
            "<ipython-input-16-5673043bdd05>:32: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n",
            "<ipython-input-21-0d040d5692bd>:8: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  train_metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n",
            "<ipython-input-14-b24f61c18315>:50: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
            "<ipython-input-14-b24f61c18315>:51: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  metrics = jax.tree_map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
            "<ipython-input-14-b24f61c18315>:29: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  minibatch = jax.tree_map(\n",
            "<ipython-input-14-b24f61c18315>:45: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  carry = jax.tree_map(jnp.add, carry, (step_grads, step_metrics))\n",
            "<ipython-input-14-b24f61c18315>:57: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  grads = jax.tree_map(lambda g: g / num_minibatches, grads)\n",
            "<ipython-input-16-5673043bdd05>:32: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  metrics = jax.tree_map(jnp.add, metrics, step_metrics)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Minibatch 1 ===\n",
            "accuracy: 0.026953\n",
            "loss: 4.593171\n",
            "== Minibatch 4 ===\n",
            "accuracy: 0.026953\n",
            "loss: 4.593171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JAX-Specific Structures\n",
        "\n",
        "In JAX, we can also use some JAX-specific structures to reduce the memory footprint of the model and help training larger models. These may not be useful for other frameworks like PyTorch, but good to keep in mind for JAX users. We cover two aspects: donating buffers and scanning.\n",
        "\n",
        "### Donating buffers\n",
        "\n",
        "In JAX, we follow the idea of functional programming where all functions need to be stateless and pure. This means that we cannot modify the input arguments, and we cannot modify other global variables. This is also true for the model parameters, which are passed as arguments to the training step and returned with updated values. This enforces the device to have memory for at least twice the model parameters and optimizer state. However, as the model grows in size, this can become a significant limitation. To mitigate this, JAX provides a mechanism to donate buffers, which allows us to reuse the memory of the input arguments for the output arguments. This can be useful when the input and output arguments have the same shape and data type, and we do not need the input arguments after the function has been executed. This is often the case for the model parameters and optimizer state, where we do not need the input arguments after the optimizer step has been executed. We can use the `jax.jit` function with the `donate_argnums`/`donate_argnames` argument to donate buffers. We can donate buffers for the model parameters and optimizer state, which can reduce the memory footprint of the model and help training larger models. We implement this below for the training step\n"
      ],
      "metadata": {
        "id": "6QmC0oIPafMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_step_donated = jax.jit(\n",
        "    train_step,\n",
        "    static_argnames=\"num_minibatches\",\n",
        "    donate_argnames=(\n",
        "        \"state\",\n",
        "        \"metrics\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "HLQdMP4gaUXw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate Summary\n",
        "\n",
        "In this notebook, we have discussed several techniques to train larger models on a single device. We have implemented mixed precision training, gradient accumulation, and gradient checkpointing on a simple MLP model. We have also discussed JAX-specific structures to reduce the memory footprint of the model and help training larger models. In the next part ([Part 1.2](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/single_gpu_transformer.html)), we will combine these techniques to train a larger Transformer model on a single GPU, and explore the benefits and trade-offs of each technique. We will also profile the model to get further insights into the efficiency of these techniques."
      ],
      "metadata": {
        "id": "-obmAwJlazzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Resources\n",
        "\n",
        "\\[Chen et al., 2016\\] Chen, T., Xu, B., Zhang, C. and Guestrin, C., 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174. [Paper link](https://arxiv.org/abs/1604.06174)\n",
        "\n",
        "\\[Micikevicius et a., 2018\\] Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G. and Wu, H., 2018, February. Mixed Precision Training. In International Conference on Learning Representations. [Paper link](https://arxiv.org/abs/1710.03740)\n",
        "\n",
        "\\[Bulatov, 2018\\] Bulatov, Y., 2018. Fitting larger networks into memory. [Blog post link](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)\n",
        "\n",
        "\\[Kalamkar et al., 2019\\] Kalamkar, D., Mudigere, D., Mellempudi, N., Das, D., Banerjee, K., Avancha, S., Vooturi, D.T., Jammalamadaka, N., Huang, J., Yuen, H. and Yang, J., 2019. A study of BFLOAT16 for deep learning training. arXiv preprint arXiv:1905.12322. [Paper link](https://arxiv.org/abs/1905.12322)\n",
        "\n",
        "\\[Ahmed et al., 2022\\] Ahmed, S., Sarofeen, C., Ruberry, M., et al., 2022. What Every User Should Know About Mixed Precision Training in PyTorch. [Tutorial link](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/)\n",
        "\n",
        "\\[Weng et al., 2022\\] Weng, L., Brockman, G., 2022. Techniques for training large neural networks. [Blog link](https://openai.com/research/techniques-for-training-large-neural-networks)\n",
        "\n",
        "\\[Raschka, 2023\\] Raschka, S., 2023. Optimizing Memory Usage for Training LLMs and Vision Transformers in PyTorch. [Tutorial link](https://lightning.ai/pages/community/tutorial/pytorch-memory-vit-llm/) (gives more details for the topics here in PyTorch)\n",
        "\n",
        "\\[HuggingFace, 2024\\] HuggingFace, 2024. Performance and Scalability: How To Fit a Bigger Model and Train It Faster. [Tutorial link](https://huggingface.co/docs/transformers/v4.18.0/en/performance)\n",
        "\n",
        "\\[NVIDIA, 2024\\] NVIDIA, 2024. Mixed Precision Training. [Documentation link](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)\n",
        "\n",
        "\\[NVIDIA, 2024\\] NVIDIA, 2024. Performance Guide for Training. [Documentation link](https://docs.nvidia.com/deeplearning/performance/index.html)\n",
        "\n",
        "\\[Google, 2024\\] JAX Team Google, 2024. Control autodiff’s saved values with jax.checkpoint (aka jax.remat). [Tutorial link](https://jax.readthedocs.io/en/latest/notebooks/autodiff_remat.html)\n",
        "\n",
        "\\[Google, 2024\\] JAX Team Google, 2024. Profiling JAX programs. [Tutorial link](https://jax.readthedocs.io/en/latest/profiling.html)\n",
        "\n",
        "\\[Google, 2024\\] JAX Team Google, 2024. GPU peformance tips. [Tutorial link](https://jax.readthedocs.io/en/latest/gpu_performance_tips.html)"
      ],
      "metadata": {
        "id": "0yHy5ZEubOoc"
      }
    }
  ]
}